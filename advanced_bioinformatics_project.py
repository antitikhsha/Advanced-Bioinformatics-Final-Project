# -*- coding: utf-8 -*-
"""Advanced_Bioinformatics_project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-e4Tj-Zg2sqVwLRvtsVgjTTpF6ErnfEm
"""

import numpy as np
import pandas as pd

"""# **Looking at the raw dataset**"""

data=pd.read_csv("healthcare-dataset-stroke-data.csv")

data.head()

data.shape

data.describe()

data.dtypes

data.columns

data.size

data.info()

"""# **Explorartory Data Analysis & Feature Engineering**"""

!pip install dataprep

!pip install pandas-profiling

from dataprep.eda import plot
from dataprep.eda import plot_correlation
from dataprep.eda import plot_missing

data=pd.read_csv("healthcare-dataset-stroke-data.csv")
data

data.describe()

#drop id
data.drop(columns=['id'],inplace=True)

#checking missing values
data.isna()

#getting the count of null values in a column
data.isna().sum()

#checking if we have missing data
plot_missing(data)

data=data.fillna(np.mean(data['bmi']))
data.info()

plot(data)

plot(data,'stroke')

plot(data,'smoking_status')

plot(data,'bmi')

plot(data,'heart_disease')

plot_correlation(data)

#converting Marrital Status, Residence and Gender into 0s and 1s
data['gender']=data['gender'].apply(lambda x : 1 if x=='Female' else 0) 
data["Residence_type"]=data["Residence_type"].apply(lambda x: 1 if x=="Urban" else 0)
data["ever_married"]=data["ever_married"].apply(lambda x: 1 if x=="Yes" else 0)

#removing the observations that have smoking_status type unknown 
data=data[data['smoking_status']!='Unknown']

data.head(12)

data

#using OneHotEncoding for smoking_status, work_type
data_dummies=data[['smoking_status','work_type']]
data_dummies=pd.get_dummies(data_dummies)
data.drop(columns=['smoking_status','work_type'],inplace=True)

data_dummies

data

y=data['stroke']
data.drop(columns=['stroke'],inplace=True)
x=data.merge(data_dummies,left_index=True, right_index=True,how='left')

"""# **Spliting Model into Training & Testing Model**"""

from sklearn.model_selection import train_test_split
X_train,X_test,Y_train,Y_test=train_test_split(x,y,test_size=0.20,random_state=0)

print(X_train.shape)
print(X_test.shape)
print(Y_train.shape)
print(Y_test.shape)

from sklearn.preprocessing import StandardScaler
sc=StandardScaler()
X_train=sc.fit_transform(X_train)
X_test=sc.transform(X_test)

X_train

Y_train

Y_test

"""# **(i) KNN**"""

from sklearn.neighbors import KNeighborsClassifier

knn=KNeighborsClassifier()
knn.fit(X_train,Y_train)

y_pred_knn=knn.predict(X_test)

y_pred_knn

"""# **(ii) SVM**"""

from sklearn.svm import SVC

svm=SVC()
svm.fit(X_train,Y_train)

y_pred_svm=svm.predict(X_test)

y_pred_svm

"""# **(iii) Decision Tree**"""

from sklearn.tree import DecisionTreeClassifier

dtree = DecisionTreeClassifier(criterion='gini',max_depth=None)

dtree.fit(X_train,Y_train)

y_pred_dtree=dtree.predict(X_test)

y_pred_dtree

"""# **(iv)Random Forest**"""

from sklearn.ensemble import RandomForestClassifier

rfc=RandomForestClassifier(n_estimators=500)

rfc.fit(X_train,Y_train)

y_pred_rfc=rfc.predict(X_test)

y_pred_rfc

"""# **(v)XGBoost**"""

from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score

model = XGBClassifier()
#fit the model with the training data
model.fit(X_train,Y_train)

y_pred_model=model.predict(X_test)

y_pred_model

"""# **(vi) Naive Bayes**"""

from sklearn.naive_bayes import GaussianNB

gnb=GaussianNB()
 y_pred = gnb.fit(X_train, Y_train).predict(X_test)

y_pred

"""# **Training Accuracy of all the Algorithms**"""

print('K Nearest Neighbor Training Accuracy:',knn.score(X_train,Y_train)*100)

print('SVM Training Accuracy:',svm.score(X_train,Y_train)*100)

print('Decision Tree Training Accuracy:',dtree.score(X_train,Y_train)*100)

print('Random Forest Training Accuracy:',rfc.score(X_train,Y_train)*100)

print('XGBoost Training Accuracy:',model.score(X_train,Y_train)*100)

print('Naive Bayes Training Accuracy:',gnb.score(X_train,Y_train)*100)

"""# **Test Accuracy of all the algorithms**"""

print('K Nearest Neighbor Training Accuracy:',knn.score(X_test,Y_test)*100)

print('SVM Training Accuracy:',svm.score(X_test,Y_test)*100)

print('Decision Tree Training Accuracy:',dtree.score(X_test,Y_test)*100)

print('Random Forest Training Accuracy:',rfc.score(X_test,Y_test)*100)

print('XGBoost Training Accuracy:',model.score(X_test,Y_test)*100)

print('Naive Bayes Accuracy:',gnb.score(X_test,Y_test)*100)

"""# **Accuracy Score of all the Algorithms**"""

from sklearn.metrics import accuracy_score

accuracy_test = accuracy_score(Y_test,y_pred_knn)

print('KNN accuracy_score on test dataset:',(accuracy_test)*100)

accuracy_test = accuracy_score(Y_test,y_pred_svm)

print('SVM accuracy_score on test dataset:',(accuracy_test)*100)

accuracy_test = accuracy_score(Y_test,y_pred_dtree)

print('Decision Tree accuracy_score on test dataset:',(accuracy_test)*100)

accuracy_test = accuracy_score(Y_test,y_pred_rfc)

print('Random Forest accuracy_score on test dataset:',(accuracy_test)*100)

accuracy_test = accuracy_score(Y_test,y_pred_model)

print('XGBoost accuracy_score on test dataset:',(accuracy_test)*100)

accuracy_test = accuracy_score(Y_test,y_pred)

print('Naive Bayes accuracy_score on test dataset:',(accuracy_test)*100)

"""## **Errors**"""

import sklearn.metrics as metrics

mae_knn=metrics.mean_absolute_error(Y_test,y_pred_knn)
mse_knn=metrics.mean_squared_error(Y_test,y_pred_knn)
rmse_knn=np.sqrt(mse_knn)

print('Mean Absolute for KNN is:',mae_knn)
print('Mean Squared Error for KNN is',mse_knn)
print('Root Mean Squared Error for KNN is:',rmse_knn)

mae_svm=metrics.mean_absolute_error(Y_test,y_pred_svm)
mse_svm=metrics.mean_squared_error(Y_test,y_pred_svm)
rmse_svm=np.sqrt(mse_svm)

print('Mean Absolute Error for SVM is:',mae_svm)
print('Mean Squared Error for SVM is:',mse_svm)
print('Root Mean Squared Error for SVM is:',rmse_svm)

mae_dtree=metrics.mean_absolute_error(Y_test,y_pred_dtree)
mse_dtree=metrics.mean_squared_error(Y_test,y_pred_dtree)
rmse_dtree=np.sqrt(mse_dtree)

print('Mean Absolute Error for Decision Tree is:',mae_dtree)
print('Mean Squared Error for Decision Tree is:',mse_dtree)
print('Root Mean Squared Error for Decision Tree is',rmse_dtree)

mae_rfc=metrics.mean_absolute_error(Y_test,y_pred_rfc)
mse_rfc=metrics.mean_squared_error(Y_test,y_pred_rfc)
rmse_rfc=np.sqrt(mse_rfc)

print('Mean Absolute Error for Random Forest is:',mae_rfc)
print('Mean Squared Error for Random Forest is:',mse_rfc)
print('Root Mean Squared Error for Random Forest is:',rmse_rfc)

mae_model=metrics.mean_absolute_error(Y_test,y_pred_model)
mse_model=metrics.mean_squared_error(Y_test,y_pred_model)
rmse_model=np.sqrt(mse_model)

print('Mean Absolute Error for XGBoost is:',mae_model)
print('Mean Squared Error for XGBoost is:',mse_model)
print('Root Mean Squared Error for XGBoost is:',rmse_model)

mae_gnb=metrics.mean_absolute_error(Y_test,y_pred)
mse_gnb=metrics.mean_squared_error(Y_test,y_pred)
rmse_gnb=np.sqrt(mse_gnb)

print('Mean Absolute Error for Naive Bayes is:',mae_gnb)
print('Mean Squared Error for Naive Bayes is:',mse_gnb)
print('Root Mean Squared Error for Naive Bayes is:',rmse_gnb)